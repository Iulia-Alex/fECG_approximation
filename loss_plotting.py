import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

# Data
epochs = list(range(1, 201))
train_loss = [0.05297714697454518, 0.017927026270124, 0.011386067598937004, 0.008878696172278586, 0.00732223980689073, 0.006059872861042983, 0.005319587999909389, 0.004945544255291664, 0.004459325801505427, 0.0042354814761234975, 0.004060006636096452, 0.0038848376825330704, 0.003796688025453832, 0.0036189824785673765, 0.00359977071733797, 0.003443528540296162, 0.003447545247485604, 0.003285533945432039, 0.0032230506696533864, 0.0031546253684484136, 0.0030882438092030644, 0.003021967389083248, 0.0029496962954176633, 0.0029575031201499024, 0.0028931072958540624, 0.002833820353224268, 0.002870728808826184, 0.0028271040409742817, 0.0026900315244598842, 0.002647147171305326, 0.0026274060535176496, 0.002541211276595307, 0.0025466327759914283, 0.002459006410516132, 0.002512304087707061, 0.002433916601178244, 0.002357483461945946, 0.0022850992665361097, 0.002254601378690421, 0.002179396351064911, 0.0021458957079283107, 0.002157731999158556, 0.0020708549688229472, 0.002004758152936593, 0.0019991222297681905, 0.0019232379878318406, 0.0018829541788171463, 0.0019034742173261758, 0.0018046548063828936, 0.001769291426289857, 0.0017262552268575968, 0.0016812799343218405, 0.0016815010051250579, 0.0016294337934019362, 0.0015916396736932116, 0.0015754707136789595, 0.001532766968958322, 0.0015119000345589668, 0.0014889363248688661, 0.0014600483180076732, 0.0014747359138739303, 0.0014428132813711597, 0.0014411583874086903, 0.0013959336177847249, 0.0013806531612753747, 0.001356563851714316, 0.001347376707547564, 0.0013241451498635902, 0.0013227774411424752, 0.001324713028535792, 0.001269880873416134, 0.0012589249990850203, 0.0012358074877183976, 0.0012174518286757838, 0.0012268874258544993, 0.0012130598637949282, 0.0011704449032668054, 0.0011454927332782588, 0.0011400969753920184, 0.0011386084711042846, 0.0011414210378321084, 0.001144237913819772, 0.0011038655255224585, 0.0011153335165330852, 0.0011078718343628495, 0.0010791790309150106, 0.0010550868601871396, 0.001055036283881835, 0.0010438551785560642, 0.0010219158853661662, 0.001030494349656581, 0.0010221183467542798, 0.0010088607768255582, 0.0009965458799653301, 0.0009868047351577903, 0.0009874995765692697, 0.0009899807604582512, 0.0009934756681474486, 0.0009634235468730936, 0.0009777840871432024, 0.0009484215084902518, 0.0009555454901608873, 0.0009281916470017011, 0.0009100449193889896, 0.0009020438022169519, 0.0008905096068539149, 0.0008976330334222232, 0.0009062483090892192, 0.0008989417517550169, 0.0008755577188666637, 0.0008713460273707543, 0.0008569838823674898, 0.0008441016581515229, 0.0008487039173733656, 0.0008509558960568251, 0.0008668549409953923, 0.0008801092295263841, 0.0008577010263045266, 0.0008205332300186581, 0.0008000686075689831, 0.000796344861903084, 0.0008002108230815852, 0.0008089465341848753, 0.0008253852223477713, 0.0008384805187825265, 0.0008156743213890227, 0.0007921017799147866, 0.0007749203425960812, 0.0007807288950165295, 0.0007860530257732342, 0.0007747277519022062, 0.0007644097455133631, 0.0007502280679205811, 0.0007589097869226603, 0.0007514085443652561, 0.0007571872220586652, 0.0007833224925689581, 0.0007744611811642422, 0.0007450716433728613, 0.0007554136843200986, 0.0007448045019158806, 0.0007233194348107024, 0.0007075188587783132, 0.0006980964406658479, 0.0007072080492715888, 0.0007069067375514869, 0.0007043303140984257, 0.0007072043097843, 0.0007133194992159744, 0.0007591566848909346, 0.0007159360490428904, 0.0007001693281004341, 0.0006876428307760962, 0.0007028827364215763, 0.0006921955216789572, 0.0006724284541588731, 0.0006721788166446718, 0.0006745755189757158, 0.0006717742511593714, 0.0006736868195523455, 0.000680669744184801, 0.0006819216972709126, 0.0006611447377183391, 0.0006584115757046616, 0.0006607064207811363, 0.0006742495996120378, 0.0006722276855252562, 0.0006710019736830903, 0.0006657470371649881, 0.0006499488651563179, 0.0006478926184874907, 0.0006450597065258619, 0.000637688305384169, 0.000625418461253488, 0.000636600879327644, 0.0006252723231124199, 0.000627028623338395, 0.0006220707692569349, 0.0006408885905019394, 0.0006572360440717662, 0.0006309666479672721, 0.0006298126280875649, 0.0006267152607395518, 0.0006371887961054236, 0.0006233991177684469, 0.0006068065252374645, 0.0006022312459763048, 0.0005979225218364196, 0.0006102339929628481, 0.0006133062912650951, 0.0006215306821400375, 0.0005992989231381808, 0.0005981760190390959, 0.0005883384144361457, 0.0005951662674712819, 0.0005989154129728251, 0.0005942345620511569, 0.0005827353158927848, 0.000592642079005245, 0.0005998316043950012]
val_loss = [0.465442076990647, 0.1575017308018037, 0.10003473676208939, 0.07800568779930472, 0.0643311068748257, 0.05324031156487763, 0.04673638028491821, 0.04345013881434819, 0.039178362398940535, 0.037211730111656446, 0.035670058302847404, 0.03413107392511198, 0.0333566162236301, 0.03179534606169909, 0.03162655701661216, 0.030253857889744853, 0.03028914753148066, 0.028865762520581484, 0.028316802311954752, 0.02771563716565392, 0.027132427752284066, 0.02655014206123139, 0.025915188881169473, 0.02598377741274557, 0.025418014099289264, 0.024897135960470353, 0.02522140310611576, 0.024838128359988332, 0.023633848393468986, 0.023257078719325364, 0.02308363889876221, 0.02232635621580162, 0.022373987960496118, 0.02160412774953459, 0.022072385913426324, 0.021383695853208855, 0.020712176129953668, 0.020076229270281538, 0.019808283541351557, 0.019147553655784577, 0.018853226576798728, 0.018957216849750175, 0.018193940083230182, 0.01761323234365721, 0.017563716732963388, 0.0168970194645226, 0.016543097428179214, 0.0167233806236514, 0.015855181513221135, 0.0155444889595466, 0.015166385207391744, 0.01477124513725617, 0.014773187402170151, 0.014315739756317012, 0.013983691418876074, 0.013841635555893714, 0.013466452655848116, 0.013283121732196637, 0.013081369139919323, 0.01282756736535313, 0.012956608386178101, 0.012676145257760904, 0.012661605832233493, 0.012264273927680083, 0.012130024202633649, 0.01191838241149006, 0.011837666787739311, 0.011633560959515827, 0.011621544661466032, 0.011638550179278744, 0.011156810530727463, 0.011060555349104106, 0.010857451499240207, 0.010696183923365814, 0.010779082384293101, 0.010657597374769725, 0.010283194507272648, 0.010063971870944701, 0.010016566283801305, 0.010003488710416215, 0.010028199118096381, 0.010052947385702282, 0.009698247117090173, 0.009799001609540678, 0.009733445401902177, 0.009481358628753307, 0.009269691700215585, 0.009269247351247551, 0.00917101335445685, 0.00897826099285989, 0.009053628929125677, 0.008980039760769745, 0.008863562539253118, 0.008755367373981113, 0.0086697844588863, 0.008675889137001442, 0.008697688109740349, 0.008728393370152585, 0.008464364018956465, 0.008590531622758135, 0.008332560396021498, 0.008395149663556367, 0.008154826612943518, 0.007995394648917551, 0.007925099119477506, 0.007823762974502253, 0.007886347365066675, 0.007962038715569568, 0.007897845390419076, 0.00769239995861426, 0.007655397240471627, 0.007529215537942946, 0.007416035996616951, 0.007456470131208854, 0.007476255372499249, 0.0076159398387452325, 0.007732388230838946, 0.007535516159675483, 0.007208970520878211, 0.007029174195070352, 0.006996458429577095, 0.00703042365993107, 0.007107173121767119, 0.007251598739198276, 0.0073666502721607685, 0.007166281537917841, 0.006959179923537054, 0.006808228724236999, 0.0068592610062166515, 0.006906037297864843, 0.006806536677426526, 0.006715885621295976, 0.006591289453873677, 0.006667564556534801, 0.006601660782637607, 0.00665243059380113, 0.006882047613284418, 0.006804194663085842, 0.006545986581061568, 0.0066368487979551515, 0.006543639552546665, 0.006354877891551171, 0.006216058544980895, 0.006133275871564235, 0.006213327861457531, 0.0062106806227737775, 0.006188044902436169, 0.006213295007390636, 0.006267021314540345, 0.006669733731541783, 0.006290009573733967, 0.0061514876683109575, 0.006041433441818559, 0.006175326898560992, 0.006081432083322268, 0.005907764275824385, 0.005905571031949616, 0.005926627773858074, 0.005902016635185906, 0.005918819914638464, 0.0059801698953378946, 0.005991169197451589, 0.005808628767096836, 0.005784615986548098, 0.005804777839719983, 0.005923764339448618, 0.005906000379971894, 0.005895231625930007, 0.005849063255092395, 0.00571026502958765, 0.005692199433854382, 0.00566731027876293, 0.005602547254446628, 0.005494747909584216, 0.005592993439807158, 0.0054934639816305465, 0.005508894333615899, 0.005465336044185928, 0.005630664045124182, 0.00577428810148766, 0.00554349269285532, 0.005533353803912178, 0.005506141219354633, 0.005598158708640507, 0.005477006534679926, 0.005331228757443439, 0.0052910316610775355, 0.005253176441848544, 0.0053613415096021655, 0.005388333844686193, 0.005460590993087473, 0.005265269110428302, 0.005255403595843485, 0.005168973212546136, 0.005228960778497692, 0.00526189969968982, 0.005220775080878022, 0.005119745989629466, 0.005206783979831796, 0.005269949095756081]
print(len(epochs))
print(len(train_loss))
print(len(val_loss))

# DataFrame-ului
data = pd.DataFrame({
    'Epoch': epochs,
    'Train Loss': train_loss,
    'Validation Loss': val_loss
})

# Graph
plt.figure(figsize=(10, 6))
sns.lineplot(data=data, x='Epoch', y='Train Loss', label='Train Loss', marker='o')
sns.lineplot(data=data, x='Epoch', y='Validation Loss', label='Validation Loss', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss per epoch on Unet_0 model')
plt.legend()
plt.show()